{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdx7rN2jh3oQLWmPsmGsqt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek3102/Reinforcement-Learning/blob/main/Reinforcement_Learning_Algos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MvW8BY4_4x2"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from itertools import count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLearning Agent :\n",
        "\n",
        "This class implements the Q-learning algorithm, a model-free reinforcement learning algorithm.\n",
        "It uses a Q-table where states are mapped to action-value pairs.\n",
        "The agent selects actions based on an epsilon-greedy policy (balance between exploration and exploitation).\n",
        "The Q-value is updated using the Bellman equation: Q(s, a) ← Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))."
      ],
      "metadata": {
        "id": "VIMjqeKoAqpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, done):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        target = reward + (self.gamma * self.q_table[next_state][best_next_action] if not done else 0)\n",
        "        self.q_table[state][action] += self.lr * (target - self.q_table[state][action])\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "4lKWNZ4hABYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSAAgent :\n",
        "SARSA (State-Action-Reward-State-Action) is an on-policy algorithm similar to Q-learning, but it updates the Q-value based on the action actually taken, rather than the greedy action.\n",
        "The agent follows an epsilon-greedy policy and updates its Q-values using the equation: Q(s, a) ← Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))."
      ],
      "metadata": {
        "id": "HCz-lR_NA7Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSAAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, next_action, done):\n",
        "        target = reward + (self.gamma * self.q_table[next_state][next_action] if not done else 0)\n",
        "        self.q_table[state][action] += self.lr * (target - self.q_table[state][action])\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
      ],
      "metadata": {
        "id": "pP-StJZuAJfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN (Deep Q-Network):\n",
        "\n",
        "DQN is an extension of Q-learning where a neural network is used to approximate the Q-value function instead of a table.\n",
        "The agent interacts with the environment and stores experiences in a replay buffer.\n",
        "During training, a batch of experiences is sampled from the buffer, and the Q-values are updated based on the Bellman equation.\n",
        "A target network is used to stabilize the learning by periodically copying the weights of the policy network."
      ],
      "metadata": {
        "id": "77vZ4YjxBL4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "pQWo-1a9APCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, env, learning_rate=1e-4, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01, batch_size=64):\n",
        "        self.env = env\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.policy_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "        self.target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).argmax(dim=1).view(1, 1)\n",
        "\n",
        "    def push_to_memory(self, state, action, next_state, reward, done):\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "    def sample_from_memory(self):\n",
        "        return random.sample(self.memory, self.batch_size)\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        transitions = self.sample_from_memory()\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s, m in zip(batch.next_state, non_final_mask) if m])\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
        "\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "lvBFCQ6wASXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "n_episodes = 1000"
      ],
      "metadata": {
        "id": "LczrpGYzAaS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_agent = QLearningAgent(env)\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = q_agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        q_agent.update_q_table(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    q_agent.decay_epsilon()"
      ],
      "metadata": {
        "id": "vgv9_Rz_AdZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_agent = SARSAAgent(env)\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    action = sarsa_agent.select_action(state)\n",
        "    done = False\n",
        "    while not done:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_action = sarsa_agent.select_action(next_state)\n",
        "        sarsa_agent.update_q_table(state, action, reward, next_state, next_action, done)\n",
        "        state, action = next_state, next_action\n",
        "    sarsa_agent.decay_epsilon()"
      ],
      "metadata": {
        "id": "yPheimxCAf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_agent = DQNAgent(env)\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = dqn_agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        dqn_agent.push_to_memory(state, action, next_state, reward, done)\n",
        "        dqn_agent.update()\n",
        "        state = next_state\n",
        "    dqn_agent.decay_epsilon()\n",
        "    if episode % 10 == 0:\n",
        "        dqn_agent.update_target()"
      ],
      "metadata": {
        "id": "kC1bFWgkAjb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction with the environment: The agent selects actions and receives rewards and next states from the environment.\n",
        "\n",
        "Updating Q-values: The Q-values (or approximations via neural networks in DQN) are updated after every step using the appropriate algorithm.\n",
        "\n",
        "Exploration and Exploitation: The epsilon-greedy strategy ensures that the agent explores the environment and gradually shifts to exploitation as learning progresses."
      ],
      "metadata": {
        "id": "JfkWTFoQBVMD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rC2OaWugBV6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}